# 🔮 Demo Queries - The Headless Horseman's Quest

Curated list of queries designed to showcase different capabilities of the AI Spirit Medium.

---

## 🎯 Primary Demo Queries (Use These!)

### Query 1: Bug Discovery
**Question:** "What bugs are mentioned?"

**Purpose:** Demonstrates file aggregation and summarization

**Expected Results:**
- Should identify 2-3 bug reports
- Mobile crashes (critical severity)
- Haunted UI flickering issue
- Performance problems
- May mention specific files by name

**Why This Works:**
- Simple, clear question
- Tests across multiple folders (product-logs)
- Shows AI can read and summarize
- Fast response time (~5-10 seconds)

**Backup:** "What bugs are in the product logs?"

---

### Query 2: Feedback Synthesis
**Question:** "Summarize user feedback"

**Purpose:** Demonstrates multi-file aggregation and synthesis

**Expected Results:**
- Pulls from feedback/ folder (5 files)
- Mentions user complaints (Sept & Oct)
- References praise and kudos
- Survey results (Q3)
- Support ticket themes
- Balanced positive/negative summary

**Why This Works:**
- Tests cross-file synthesis
- Shows AI can aggregate scattered info
- Perfect "reunite the lost head" metaphor
- Engaging results (real user sentiment)

**Backup:** "What are users saying in the feedback folder?"

---

### Query 3: Feature Requests
**Question:** "What features are users requesting?"

**Purpose:** Demonstrates AI extraction and prioritization

**Expected Results:**
- Dark mode feature (highest priority)
- Mobile improvements
- Performance enhancements
- UI fixes
- May rank by frequency or urgency

**Why This Works:**
- Action-oriented results
- Tests extraction of specific info types
- Shows AI understands context (requests vs complaints)
- Relevant to product development scenario

**Backup:** "What do users want us to build?"

---

## 🚀 Advanced Queries (If Extra Time)

### Query 4: Sprint Analysis
**Question:** "What did the sprint retrospective say about bugs?"

**Purpose:** Demonstrates file-specific deep dive

**Expected Results:**
- References sprint-retrospective-oct.md
- Mentions bug triage process
- Team reflections on bug handling
- May connect to other bug reports

**Why This Works:**
- Tests specific file targeting
- Shows AI can do deep analysis
- Connects retrospective to actual bugs
- Demonstrates wikilink-style thinking

---

### Query 5: Problem Prioritization
**Question:** "What are the biggest problems right now?"

**Purpose:** Demonstrates AI judgment and prioritization

**Expected Results:**
- Mobile crashes (critical)
- Performance issues (high impact)
- UI bugs (user-facing)
- May rank by severity/frequency
- Could mention user impact

**Why This Works:**
- Tests AI's ability to prioritize
- Shows reasoning capability
- Aggregates multiple sources
- Actionable insights

---

### Query 6: Temporal Analysis
**Question:** "How has user sentiment changed from September to October?"

**Purpose:** Demonstrates temporal reasoning

**Expected Results:**
- Compares user-complaints-sept.md vs oct.md
- May note trend improvements or deterioration
- References specific complaint themes
- Shows month-over-month analysis

**Why This Works:**
- Advanced reasoning
- Cross-file temporal comparison
- Shows AI understands dates
- Impressive analytical capability

---

## 🎨 Creative Queries (Wow Factor)

### Query 7: Meeting Context
**Question:** "What was discussed in the rambling meeting notes?"

**Purpose:** Tests AI's ability to extract signal from noise

**Expected Results:**
- Extracts key points from scattered notes
- Identifies action items
- Connects to other files mentioned
- Shows AI can parse unstructured content

---

### Query 8: Inspiration Extraction
**Question:** "What ideas are in the inspiration graveyard?"

**Purpose:** Fun, on-theme query

**Expected Results:**
- Lists product ideas
- May categorize by type
- Could connect to feature requests
- Shows AI can extract creative content

---

### Query 9: Todo Analysis
**Question:** "What urgent tasks are mentioned across all files?"

**Purpose:** Demonstrates extraction across file types

**Expected Results:**
- Finds todo items
- Bug fixes marked urgent
- Feature requests flagged critical
- May prioritize by frequency

---

### Query 10: Tag-Based Query
**Question:** "Show me everything tagged with 'mobile'"

**Purpose:** Tests tag awareness and filtering

**Expected Results:**
- Lists files with mobile tag
- Summarizes mobile-related content
- Bug reports about mobile crashes
- Feature requests for mobile

---

## 🎯 Query Strategy by Demo Phase

### Phase 1: Warm-Up (Establish Basics)
- Start with Query 1: "What bugs are mentioned?"
- Simple, fast, reliable
- Builds confidence

### Phase 2: Main Demo (Show Power)
- Use Query 2: "Summarize user feedback"
- Most impressive aggregation
- Perfect for "reunite" metaphor

### Phase 3: Closer (If Time)
- Query 3: "What features are users requesting?"
- Action-oriented
- Leaves judges with clear takeaway

### Phase 4: Q&A (Judge Questions)
- Have Queries 4-10 ready
- Adapt to judge interests
- Show versatility

---

## 🎬 Query Delivery Tips

### Before Typing
1. **Announce** what you're going to ask
2. **Explain** what you expect to see
3. **Type** the query
4. **Hit Enter** confidently

### During Streaming
1. **Stay silent** - let AI work
2. **Point** to results as they appear
3. **Nod** to show satisfaction
4. **Smile** - you're impressed too!

### After Response
1. **Highlight** key points in the answer
2. **Point out** file citations (if present)
3. **Connect** to the demo narrative
4. **Transition** to next section

---

## 🚨 Failure Recovery Queries

If a query fails or returns poor results:

### Fallback Query A
**"What files mention bugs?"**
- Simpler extraction task
- Lists file names
- Shows discovery capability

### Fallback Query B
**"Summarize the mobile bug report"**
- Specific file targeting
- Simpler task
- Still demonstrates capability

### Fallback Query C
**"List all the tags in the vault"**
- Metadata query
- Always works
- Shows organization

---

## 📊 Query Performance Matrix

| Query | Speed | Complexity | Wow Factor | Risk |
|-------|-------|------------|------------|------|
| Query 1: Bugs | ⚡ Fast | 🟢 Low | ⭐⭐⭐ | 🟢 Safe |
| Query 2: Feedback | ⚡ Fast | 🟡 Med | ⭐⭐⭐⭐⭐ | 🟢 Safe |
| Query 3: Features | ⚡ Fast | 🟡 Med | ⭐⭐⭐⭐ | 🟢 Safe |
| Query 4: Sprint | 🟡 Med | 🟡 Med | ⭐⭐⭐⭐ | 🟡 Medium |
| Query 5: Problems | 🟡 Med | 🔴 High | ⭐⭐⭐⭐⭐ | 🟡 Medium |
| Query 6: Sentiment | 🔴 Slow | 🔴 High | ⭐⭐⭐⭐⭐ | 🔴 Risky |

**Legend:**
- Speed: ⚡ Fast (<10s) | 🟡 Medium (10-15s) | 🔴 Slow (15s+)
- Complexity: 🟢 Low | 🟡 Medium | 🔴 High
- Wow Factor: ⭐ (1-5 stars)
- Risk: 🟢 Safe | 🟡 Medium | 🔴 Risky

---

## 🎯 Recommended Demo Flow

**Scenario A: Full 3-Minute Demo**
1. Query 1: Bugs (establish capability)
2. Query 2: Feedback (main wow moment)
3. Query 3: Features (action-oriented close)

**Scenario B: Short 2-Minute Demo**
1. Query 1: Bugs (quick proof)
2. Query 2: Feedback (main demo)
3. Skip to closing

**Scenario C: Q&A Follow-Up**
- Use Queries 4-10 based on judge questions
- Show versatility
- Adapt to interests

---

## 💡 Query Writing Tips

### Good Query Characteristics
✅ Clear and concise (under 10 words)
✅ Uses terms present in files ("bugs", "feedback", "features")
✅ Open-ended enough for synthesis
✅ Specific enough to be answerable

### Bad Query Characteristics
❌ Too vague ("Tell me about stuff")
❌ Too specific ("What did John say on line 47?")
❌ Uses jargon not in files
❌ Requires external knowledge

### Pro Tips
- Use present tense ("are mentioned" not "were mentioned")
- Avoid yes/no questions
- Ask for summaries, not lists
- Let AI show synthesis capability

---

## 🎃 Theme-Appropriate Queries (Fun!)

If you want to lean into the spooky theme:

1. **"What spirits haunt this vault?"** (creative way to ask "what's in here?")
2. **"What dark secrets lurk in the feedback?"** (user complaints)
3. **"What curses need lifting?"** (bugs to fix)
4. **"What offerings do users demand?"** (feature requests)

*Note: These are fun but risky - use standard queries for main demo!*

---

## 📝 Query Preparation Checklist

**Before Demo:**
- [ ] Test all primary queries (1-3)
- [ ] Verify response quality
- [ ] Time each query
- [ ] Note key points in responses
- [ ] Prepare transitions between queries
- [ ] Have backups ready
- [ ] Practice pronunciation
- [ ] Memorize query text (don't read from notes!)

**During Demo:**
- [ ] Type confidently
- [ ] Don't apologize if slow
- [ ] Highlight key results
- [ ] Connect to narrative
- [ ] Transition smoothly

**After Each Query:**
- [ ] Did it answer the question? ✅
- [ ] Did it cite files? ✅
- [ ] Was it fast enough? ✅
- [ ] Did judges react? ✅

---

**Remember:** The queries are your storytelling tool. Each one should build on the last to show the power of reuniting scattered information! 🔮👻

**May your queries summon the perfect answers!** 🎃
